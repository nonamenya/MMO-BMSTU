{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df78ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df189ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NoNam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18bc9c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\"rec.sport.baseball\", \"rec.autos\", \"sci.space\", \"talk.politics.guns\"]\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
    "train = newsgroups['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "458a5d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество сформированных признаков - 36320\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vocabVect = CountVectorizer()\n",
    "vocabVect.fit(train)\n",
    "corpusVocab = vocabVect.vocabulary_\n",
    "print('Количество сформированных признаков - {}'.format(len(corpusVocab)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c01233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looper=21139\n",
      "cco=9458\n",
      "caltech=9116\n",
      "edu=13479\n",
      "mark=21794\n",
      "subject=31556\n",
      "re=27442\n",
      "command=10375\n",
      "loss=21171\n"
     ]
    }
   ],
   "source": [
    "for i in list(corpusVocab)[1:10]:\n",
    "    print('{}={}'.format(i, corpusVocab[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdce3359",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = vocabVect.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cf51071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2330x36320 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 373978 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b5c3e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4295d3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36320"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Размер нулевой строки\n",
    "len(test_features.todense()[0].getA1())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f3c5931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 17,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Непустые значения нулевой строки\n",
    "[i for i in test_features.todense()[0].getA1() if i>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9076de3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clarke',\n",
       " 'clarkson',\n",
       " 'clarku',\n",
       " 'clas',\n",
       " 'clash',\n",
       " 'class',\n",
       " 'classes',\n",
       " 'classic',\n",
       " 'classical',\n",
       " 'classification',\n",
       " 'classified',\n",
       " 'classify',\n",
       " 'classroom',\n",
       " 'claudio',\n",
       " 'clause',\n",
       " 'clauses',\n",
       " 'claw',\n",
       " 'clay',\n",
       " 'clayco',\n",
       " 'claypigeon']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabVect.get_feature_names()[10000:10020]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa398d1c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Решение задачи анализа тональности текста на основе модели \"мешка слов\"\n",
    "\n",
    "С использованием кросс-валидации попробуем применить к корпусу текстов различные варианты векторизации и классификации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "916e0150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VectorizeAndClassify(vectorizers_list, classifiers_list):\n",
    "    for v in vectorizers_list:\n",
    "        for c in classifiers_list:\n",
    "            pipeline1 = Pipeline([(\"vectorizer\", v), (\"classifier\", c)])\n",
    "            score = cross_val_score(pipeline1, newsgroups['data'], newsgroups['target'], scoring='accuracy', cv=3).mean()\n",
    "            print('Векторизация - {}'.format(v))\n",
    "            print('Модель для классификации - {}'.format(c))\n",
    "            print('Accuracy = {}'.format(score))\n",
    "            print('===========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31586858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NoNam\\anaconda4\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\NoNam\\anaconda4\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\NoNam\\anaconda4\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
      "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
      "                            '000152': 7, '00041032': 8, '0004136': 9,\n",
      "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
      "                            '0004847546': 13, '0005': 14, '0005111312': 15,\n",
      "                            '0005111312na3em': 16, '000601': 17, '000710': 18,\n",
      "                            '00090711': 19, '000mi': 20, '000miles': 21,\n",
      "                            '000s': 22, '000th': 23, '001': 24, '0010': 25,\n",
      "                            '0012': 26, '001211': 27, '001319': 28,\n",
      "                            '001428': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0)\n",
      "Accuracy = 0.9618018239152261\n",
      "===========================\n",
      "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
      "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
      "                            '000152': 7, '00041032': 8, '0004136': 9,\n",
      "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
      "                            '0004847546': 13, '0005': 14, '0005111312': 15,\n",
      "                            '0005111312na3em': 16, '000601': 17, '000710': 18,\n",
      "                            '00090711': 19, '000mi': 20, '000miles': 21,\n",
      "                            '000s': 22, '000th': 23, '001': 24, '0010': 25,\n",
      "                            '0012': 26, '001211': 27, '001319': 28,\n",
      "                            '001428': 29, ...})\n",
      "Модель для классификации - LinearSVC()\n",
      "Accuracy = 0.9673804879990447\n",
      "===========================\n",
      "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
      "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
      "                            '000152': 7, '00041032': 8, '0004136': 9,\n",
      "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
      "                            '0004847546': 13, '0005': 14, '0005111312': 15,\n",
      "                            '0005111312na3em': 16, '000601': 17, '000710': 18,\n",
      "                            '00090711': 19, '000mi': 20, '000miles': 21,\n",
      "                            '000s': 22, '000th': 23, '001': 24, '0010': 25,\n",
      "                            '0012': 26, '001211': 27, '001319': 28,\n",
      "                            '001428': 29, ...})\n",
      "Модель для классификации - KNeighborsClassifier()\n",
      "Accuracy = 0.6600724435775982\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
      "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
      "                            '000152': 7, '00041032': 8, '0004136': 9,\n",
      "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
      "                            '0004847546': 13, '0005': 14, '0005111312': 15,\n",
      "                            '0005111312na3em': 16, '000601': 17, '000710': 18,\n",
      "                            '00090711': 19, '000mi': 20, '000miles': 21,\n",
      "                            '000s': 22, '000th': 23, '001': 24, '0010': 25,\n",
      "                            '0012': 26, '001211': 27, '001319': 28,\n",
      "                            '001428': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0)\n",
      "Accuracy = 0.9781071572308685\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
      "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
      "                            '000152': 7, '00041032': 8, '0004136': 9,\n",
      "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
      "                            '0004847546': 13, '0005': 14, '0005111312': 15,\n",
      "                            '0005111312na3em': 16, '000601': 17, '000710': 18,\n",
      "                            '00090711': 19, '000mi': 20, '000miles': 21,\n",
      "                            '000s': 22, '000th': 23, '001': 24, '0010': 25,\n",
      "                            '0012': 26, '001211': 27, '001319': 28,\n",
      "                            '001428': 29, ...})\n",
      "Модель для классификации - LinearSVC()\n",
      "Accuracy = 0.9858346933089202\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
      "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
      "                            '000152': 7, '00041032': 8, '0004136': 9,\n",
      "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
      "                            '0004847546': 13, '0005': 14, '0005111312': 15,\n",
      "                            '0005111312na3em': 16, '000601': 17, '000710': 18,\n",
      "                            '00090711': 19, '000mi': 20, '000miles': 21,\n",
      "                            '000s': 22, '000th': 23, '001': 24, '0010': 25,\n",
      "                            '0012': 26, '001211': 27, '001319': 28,\n",
      "                            '001428': 29, ...})\n",
      "Модель для классификации - KNeighborsClassifier()\n",
      "Accuracy = 0.9141667440636514\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "vectorizers_list = [CountVectorizer(vocabulary = corpusVocab), TfidfVectorizer(vocabulary = corpusVocab)]\n",
    "classifiers_list = [LogisticRegression(C=3.0), LinearSVC(), KNeighborsClassifier()]\n",
    "VectorizeAndClassify(vectorizers_list, classifiers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0212633f",
   "metadata": {},
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e41614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "corpus = []\n",
    "stop_words = stopwords.words('english')\n",
    "tok = WordPunctTokenizer()\n",
    "for line in train:\n",
    "    line1 = line.strip().lower()\n",
    "    line1 = re.sub(\"[^a-zA-Z]\",\" \", line1)\n",
    "    text_tok = tok.tokenize(line1)\n",
    "    text_tok1 = [w for w in text_tok if not w in stop_words]\n",
    "    corpus.append(text_tok1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30998741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['looper',\n",
       "  'cco',\n",
       "  'caltech',\n",
       "  'edu',\n",
       "  'mark',\n",
       "  'looper',\n",
       "  'subject',\n",
       "  'command',\n",
       "  'loss',\n",
       "  'timer',\n",
       "  'galileo',\n",
       "  'update',\n",
       "  'organization',\n",
       "  'california',\n",
       "  'institute',\n",
       "  'technology',\n",
       "  'pasadena',\n",
       "  'lines',\n",
       "  'nntp',\n",
       "  'posting',\n",
       "  'host',\n",
       "  'sandman',\n",
       "  'caltech',\n",
       "  'edu',\n",
       "  'keywords',\n",
       "  'galileo',\n",
       "  'jpl',\n",
       "  'prb',\n",
       "  'access',\n",
       "  'digex',\n",
       "  'com',\n",
       "  'pat',\n",
       "  'writes',\n",
       "  'galileo',\n",
       "  'hga',\n",
       "  'stuck',\n",
       "  'hga',\n",
       "  'left',\n",
       "  'closed',\n",
       "  'galileo',\n",
       "  'venus',\n",
       "  'flyby',\n",
       "  'hga',\n",
       "  'pointed',\n",
       "  'att',\n",
       "  'sun',\n",
       "  'near',\n",
       "  'venus',\n",
       "  'would',\n",
       "  'cook',\n",
       "  'foci',\n",
       "  'elements',\n",
       "  'question',\n",
       "  'galileo',\n",
       "  'course',\n",
       "  'manuevers',\n",
       "  'designed',\n",
       "  'hga',\n",
       "  'ever',\n",
       "  'sun',\n",
       "  'point',\n",
       "  'hga',\n",
       "  'reflective',\n",
       "  'wavelengths',\n",
       "  'might',\n",
       "  'cook',\n",
       "  'focal',\n",
       "  'elements',\n",
       "  'figure',\n",
       "  'good',\n",
       "  'scales',\n",
       "  'problem',\n",
       "  'antenna',\n",
       "  'could',\n",
       "  'exposed',\n",
       "  'venus',\n",
       "  'level',\n",
       "  'sunlight',\n",
       "  'lest',\n",
       "  'like',\n",
       "  'icarus',\n",
       "  'wings',\n",
       "  'melt',\n",
       "  'think',\n",
       "  'glues',\n",
       "  'well',\n",
       "  'electronics',\n",
       "  'worried',\n",
       "  'thus',\n",
       "  'remain',\n",
       "  'furled',\n",
       "  'axis',\n",
       "  'always',\n",
       "  'pointed',\n",
       "  'near',\n",
       "  'sun',\n",
       "  'small',\n",
       "  'sunshade',\n",
       "  'tip',\n",
       "  'antenna',\n",
       "  'mast',\n",
       "  'would',\n",
       "  'shadow',\n",
       "  'folded',\n",
       "  'hga',\n",
       "  'larger',\n",
       "  'sunshade',\n",
       "  'beneath',\n",
       "  'antenna',\n",
       "  'shielded',\n",
       "  'spacecraft',\n",
       "  'bus',\n",
       "  'mark',\n",
       "  'looper',\n",
       "  'hot',\n",
       "  'rodders',\n",
       "  'america',\n",
       "  'first',\n",
       "  'recyclers'],\n",
       " ['yoony',\n",
       "  'aix',\n",
       "  'rpi',\n",
       "  'edu',\n",
       "  'young',\n",
       "  'hoon',\n",
       "  'yoon',\n",
       "  'subject',\n",
       "  'gun',\n",
       "  'talk',\n",
       "  'legislative',\n",
       "  'update',\n",
       "  'states',\n",
       "  'keywords',\n",
       "  'gun',\n",
       "  'talk',\n",
       "  'ila',\n",
       "  'nntp',\n",
       "  'posting',\n",
       "  'host',\n",
       "  'aix',\n",
       "  'rpi',\n",
       "  'edu',\n",
       "  'distribution',\n",
       "  'usa',\n",
       "  'lines',\n",
       "  'viking',\n",
       "  'iastate',\n",
       "  'edu',\n",
       "  'dan',\n",
       "  'sorenson',\n",
       "  'writes',\n",
       "  'lvc',\n",
       "  'cbnews',\n",
       "  'cb',\n",
       "  'att',\n",
       "  'com',\n",
       "  'larry',\n",
       "  'cipriani',\n",
       "  'writes',\n",
       "  'iowa',\n",
       "  'firearm',\n",
       "  'related',\n",
       "  'bills',\n",
       "  'dead',\n",
       "  'senate',\n",
       "  'file',\n",
       "  'dealing',\n",
       "  'duty',\n",
       "  'police',\n",
       "  'officers',\n",
       "  'carrying',\n",
       "  'concealed',\n",
       "  'remains',\n",
       "  'viable',\n",
       "  'power',\n",
       "  'word',\n",
       "  'processor',\n",
       "  'stamp',\n",
       "  'work',\n",
       "  'fact',\n",
       "  'around',\n",
       "  'state',\n",
       "  'rep',\n",
       "  'generally',\n",
       "  'lives',\n",
       "  'nine',\n",
       "  'miles',\n",
       "  'constituent',\n",
       "  'hurt',\n",
       "  'either',\n",
       "  'dan',\n",
       "  'sorenson',\n",
       "  'dod',\n",
       "  'z',\n",
       "  'dan',\n",
       "  'exnet',\n",
       "  'iastate',\n",
       "  'edu',\n",
       "  'viking',\n",
       "  'iastate',\n",
       "  'edu',\n",
       "  'isu',\n",
       "  'censors',\n",
       "  'read',\n",
       "  'say',\n",
       "  'blame',\n",
       "  'usenet',\n",
       "  'post',\n",
       "  'exotic',\n",
       "  'distant',\n",
       "  'machines',\n",
       "  'meet',\n",
       "  'exciting',\n",
       "  'unusual',\n",
       "  'people',\n",
       "  'flame',\n",
       "  'anyone',\n",
       "  'know',\n",
       "  'particulars',\n",
       "  'senate',\n",
       "  'file',\n",
       "  'bill',\n",
       "  'allow',\n",
       "  'deny',\n",
       "  'duty',\n",
       "  'police',\n",
       "  'carrying',\n",
       "  'concealed',\n",
       "  'information',\n",
       "  'iowa',\n",
       "  'discretionary',\n",
       "  'permit',\n",
       "  'policy',\n",
       "  'ccw',\n",
       "  'allows',\n",
       "  'police',\n",
       "  'duty',\n",
       "  'carry',\n",
       "  'concealed',\n",
       "  'would',\n",
       "  'inclined',\n",
       "  'oppose',\n",
       "  'believe',\n",
       "  'duty',\n",
       "  'police',\n",
       "  'officers',\n",
       "  'rights',\n",
       "  'civilians',\n",
       "  'law',\n",
       "  'policy',\n",
       "  'prevents',\n",
       "  'law',\n",
       "  'abiding',\n",
       "  'citizens',\n",
       "  'armed',\n",
       "  'self',\n",
       "  'defense',\n",
       "  'duty',\n",
       "  'police',\n",
       "  'officer',\n",
       "  'treated',\n",
       "  'differently'],\n",
       " ['pa',\n",
       "  'utkvm',\n",
       "  'utk',\n",
       "  'edu',\n",
       "  'david',\n",
       "  'veal',\n",
       "  'subject',\n",
       "  'boston',\n",
       "  'gun',\n",
       "  'buy',\n",
       "  'back',\n",
       "  'lines',\n",
       "  'organization',\n",
       "  'university',\n",
       "  'tennessee',\n",
       "  'division',\n",
       "  'continuing',\n",
       "  'education',\n",
       "  'article',\n",
       "  'hpfcso',\n",
       "  'fc',\n",
       "  'hp',\n",
       "  'com',\n",
       "  'ron',\n",
       "  'hpfcso',\n",
       "  'fc',\n",
       "  'hp',\n",
       "  'com',\n",
       "  'ron',\n",
       "  'miller',\n",
       "  'writes',\n",
       "  'urbin',\n",
       "  'interlan',\n",
       "  'interlan',\n",
       "  'com',\n",
       "  'mark',\n",
       "  'urbin',\n",
       "  'rm',\n",
       "  'short',\n",
       "  'thought',\n",
       "  'ask',\n",
       "  'question',\n",
       "  'authorities',\n",
       "  'sponsors',\n",
       "  'buyback',\n",
       "  'programs',\n",
       "  'whether',\n",
       "  'check',\n",
       "  'stolen',\n",
       "  'weapons',\n",
       "  'answer',\n",
       "  'total',\n",
       "  'amnesty',\n",
       "  'please',\n",
       "  'note',\n",
       "  'given',\n",
       "  'firearm',\n",
       "  'boston',\n",
       "  'buy',\n",
       "  'back',\n",
       "  'cash',\n",
       "  'money',\n",
       "  'orders',\n",
       "  'much',\n",
       "  'total',\n",
       "  'amnesty',\n",
       "  'get',\n",
       "  'leave',\n",
       "  'paper',\n",
       "  'trail',\n",
       "  'behind',\n",
       "  'latest',\n",
       "  'case',\n",
       "  'denver',\n",
       "  'giving',\n",
       "  'away',\n",
       "  'tickets',\n",
       "  'denver',\n",
       "  'nuggets',\n",
       "  'basketball',\n",
       "  'game',\n",
       "  'traceable',\n",
       "  'money',\n",
       "  'order',\n",
       "  'know',\n",
       "  'used',\n",
       "  'one',\n",
       "  'years',\n",
       "  'money',\n",
       "  'orders',\n",
       "  'operate',\n",
       "  'pretty',\n",
       "  'much',\n",
       "  'like',\n",
       "  'checks',\n",
       "  'parties',\n",
       "  'supposed',\n",
       "  'sign',\n",
       "  'assume',\n",
       "  'show',\n",
       "  'buy',\n",
       "  'back',\n",
       "  'people',\n",
       "  'id',\n",
       "  'money',\n",
       "  'order',\n",
       "  'made',\n",
       "  'id',\n",
       "  'far',\n",
       "  'traceable',\n",
       "  'practical',\n",
       "  'matter',\n",
       "  'know',\n",
       "  'would',\n",
       "  'depend',\n",
       "  'whether',\n",
       "  'bother',\n",
       "  'computerize',\n",
       "  'recipient',\n",
       "  'name',\n",
       "  'money',\n",
       "  'order',\n",
       "  'bother',\n",
       "  'keying',\n",
       "  'sort',\n",
       "  'thing',\n",
       "  'say',\n",
       "  'certainly',\n",
       "  'police',\n",
       "  'buyback',\n",
       "  'people',\n",
       "  'would',\n",
       "  'keep',\n",
       "  'record',\n",
       "  'gave',\n",
       "  'money',\n",
       "  'orders',\n",
       "  'even',\n",
       "  'issue',\n",
       "  'weapons',\n",
       "  'checked',\n",
       "  'stolen',\n",
       "  'might',\n",
       "  'questions',\n",
       "  'asked',\n",
       "  'suppose',\n",
       "  'somebody',\n",
       "  'brought',\n",
       "  'number',\n",
       "  'weapons',\n",
       "  'time',\n",
       "  'series',\n",
       "  'buy',\n",
       "  'back',\n",
       "  'programs',\n",
       "  'david',\n",
       "  'veal',\n",
       "  'univ',\n",
       "  'tenn',\n",
       "  'div',\n",
       "  'cont',\n",
       "  'education',\n",
       "  'info',\n",
       "  'services',\n",
       "  'group',\n",
       "  'pa',\n",
       "  'utkvm',\n",
       "  'utk',\n",
       "  'edu',\n",
       "  'still',\n",
       "  'remember',\n",
       "  'way',\n",
       "  'laughed',\n",
       "  'day',\n",
       "  'pushed',\n",
       "  'elevator',\n",
       "  'shaft',\n",
       "  'beginning',\n",
       "  'think',\n",
       "  'love',\n",
       "  'anymore',\n",
       "  'weird',\n",
       "  'al'],\n",
       " ['thatchh',\n",
       "  'hplsla',\n",
       "  'hp',\n",
       "  'com',\n",
       "  'thatch',\n",
       "  'harvey',\n",
       "  'subject',\n",
       "  'removing',\n",
       "  'rain',\n",
       "  'x',\n",
       "  'coat',\n",
       "  'front',\n",
       "  'windshield',\n",
       "  'tips',\n",
       "  'organization',\n",
       "  'hp',\n",
       "  'lake',\n",
       "  'stevens',\n",
       "  'wa',\n",
       "  'lines',\n",
       "  'want',\n",
       "  'summer',\n",
       "  'without',\n",
       "  'rain',\n",
       "  'wrong',\n",
       "  'place',\n",
       "  'must',\n",
       "  'whole',\n",
       "  'year',\n",
       "  'yet',\n",
       "  'keep',\n",
       "  'rain',\n",
       "  'x',\n",
       "  'handy',\n",
       "  'friend',\n",
       "  'thatch',\n",
       "  'thatch',\n",
       "  'harvey',\n",
       "  'uucp',\n",
       "  'longer',\n",
       "  'valid',\n",
       "  'domain',\n",
       "  'thatchh',\n",
       "  'hplsla',\n",
       "  'hp',\n",
       "  'com',\n",
       "  'hewlett',\n",
       "  'packard',\n",
       "  'lake',\n",
       "  'stevens',\n",
       "  'instrument',\n",
       "  'division',\n",
       "  'lake',\n",
       "  'stevens',\n",
       "  'wa',\n",
       "  'merkur',\n",
       "  'xr',\n",
       "  'ti',\n",
       "  'suzuki',\n",
       "  'gsx',\n",
       "  'g',\n",
       "  'prince',\n",
       "  'sr',\n",
       "  'sports',\n",
       "  'racer'],\n",
       " ['rjwade',\n",
       "  'rainbow',\n",
       "  'ecn',\n",
       "  'purdue',\n",
       "  'edu',\n",
       "  'robert',\n",
       "  'j',\n",
       "  'wade',\n",
       "  'subject',\n",
       "  'improvements',\n",
       "  'automatic',\n",
       "  'transmissions',\n",
       "  'organization',\n",
       "  'purdue',\n",
       "  'university',\n",
       "  'engineering',\n",
       "  'computer',\n",
       "  'network',\n",
       "  'lines',\n",
       "  'article',\n",
       "  'qugvu',\n",
       "  'ai',\n",
       "  'quad',\n",
       "  'wfunet',\n",
       "  'wfu',\n",
       "  'edu',\n",
       "  'hagenjd',\n",
       "  'wfu',\n",
       "  'edu',\n",
       "  'jeff',\n",
       "  'hagen',\n",
       "  'writes',\n",
       "  'thanx',\n",
       "  'responded',\n",
       "  'particularly',\n",
       "  'never',\n",
       "  'driven',\n",
       "  'xxx',\n",
       "  'like',\n",
       "  'guys',\n",
       "  'ok',\n",
       "  'know',\n",
       "  'new',\n",
       "  'age',\n",
       "  'great',\n",
       "  'traffic',\n",
       "  'satisfactory',\n",
       "  'job',\n",
       "  'acceleration',\n",
       "  'keep',\n",
       "  'foot',\n",
       "  'buried',\n",
       "  'carpet',\n",
       "  'question',\n",
       "  'regards',\n",
       "  'downshifting',\n",
       "  'downshifting',\n",
       "  'pass',\n",
       "  'good',\n",
       "  'manually',\n",
       "  'moving',\n",
       "  'lever',\n",
       "  'point',\n",
       "  'would',\n",
       "  'downshift',\n",
       "  'manual',\n",
       "  'e',\n",
       "  'g',\n",
       "  'approaching',\n",
       "  'red',\n",
       "  'light',\n",
       "  'curve',\n",
       "  'tooling',\n",
       "  'around',\n",
       "  'parking',\n",
       "  'lot',\n",
       "  'st',\n",
       "  'nd',\n",
       "  'w',\n",
       "  'shifting',\n",
       "  'still',\n",
       "  'manual',\n",
       "  'trans',\n",
       "  'bigot',\n",
       "  'downshifting',\n",
       "  'deceleration',\n",
       "  'seems',\n",
       "  'natural',\n",
       "  'try',\n",
       "  'automatic',\n",
       "  'tranny',\n",
       "  'seem',\n",
       "  'understand',\n",
       "  'want',\n",
       "  'addendum',\n",
       "  'great',\n",
       "  'downshifting',\n",
       "  'approaching',\n",
       "  'red',\n",
       "  'light',\n",
       "  'light',\n",
       "  'goes',\n",
       "  'green',\n",
       "  'already',\n",
       "  'cam',\n",
       "  'turbo',\n",
       "  'already',\n",
       "  'spooled',\n",
       "  'zippppppppppp',\n",
       "  'hagen',\n",
       "  'hagenjd',\n",
       "  'ac',\n",
       "  'wfu',\n",
       "  'edu',\n",
       "  'grand',\n",
       "  'auto',\n",
       "  'quad',\n",
       "  'around',\n",
       "  'gear',\n",
       "  'selector',\n",
       "  'plastic',\n",
       "  'strip',\n",
       "  'covers',\n",
       "  'space',\n",
       "  'see',\n",
       "  'inside',\n",
       "  'anyway',\n",
       "  'took',\n",
       "  'cover',\n",
       "  'cut',\n",
       "  'end',\n",
       "  'long',\n",
       "  'strip',\n",
       "  'specific',\n",
       "  'length',\n",
       "  'strip',\n",
       "  'curls',\n",
       "  'cirlce',\n",
       "  'one',\n",
       "  'end',\n",
       "  'inside',\n",
       "  'anyway',\n",
       "  'strip',\n",
       "  'feed',\n",
       "  'lip',\n",
       "  'circles',\n",
       "  'push',\n",
       "  'button',\n",
       "  'pull',\n",
       "  'gear',\n",
       "  'shifter',\n",
       "  'go',\n",
       "  'back',\n",
       "  'drive',\n",
       "  'accidental',\n",
       "  'hitting',\n",
       "  'first',\n",
       "  'drive',\n",
       "  'around',\n",
       "  'town',\n",
       "  'keep',\n",
       "  'revs',\n",
       "  'shift',\n",
       "  'drive',\n",
       "  'pull',\n",
       "  'coming',\n",
       "  'lights',\n",
       "  'want',\n",
       "  'eating',\n",
       "  'food',\n",
       "  'drive',\n",
       "  'drive',\n",
       "  'probably',\n",
       "  'aftermarket',\n",
       "  'shift',\n",
       "  'kits',\n",
       "  'accomplish',\n",
       "  'thing',\n",
       "  'porsche',\n",
       "  'tip',\n",
       "  'tronic',\n",
       "  'automatic',\n",
       "  'driven',\n",
       "  'like',\n",
       "  'auto',\n",
       "  'put',\n",
       "  'mode',\n",
       "  'tip',\n",
       "  'upshift',\n",
       "  'tip',\n",
       "  'downshift',\n",
       "  'course',\n",
       "  'override',\n",
       "  'redline',\n",
       "  'engine']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e9576ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество текстов в корпусе не изменилось и соответствует целевому признаку\n",
    "assert len(train)==len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0756faf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c6ea82ec1bbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'model = word2vec.Word2Vec(corpus)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "%time model = word2vec.Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c383e4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.0.1-cp38-cp38-win_amd64.whl (23.9 MB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\nonam\\anaconda4\\lib\\site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\nonam\\anaconda4\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Collecting Cython==0.29.21\n",
      "  Downloading Cython-0.29.21-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\nonam\\anaconda4\\lib\\site-packages (from gensim) (1.20.2)\n",
      "Requirement already satisfied: requests in c:\\users\\nonam\\anaconda4\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.25.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\nonam\\anaconda4\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nonam\\anaconda4\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nonam\\anaconda4\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\nonam\\anaconda4\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Installing collected packages: Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.23\n",
      "    Uninstalling Cython-0.29.23:\n",
      "      Successfully uninstalled Cython-0.29.23\n",
      "Successfully installed Cython-0.29.21 gensim-4.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e01b5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NoNam\\anaconda4\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.55 s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "%time model = word2vec.Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3251e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pretty', 0.9931106567382812), ('going', 0.9920623302459717), ('far', 0.9914496541023254), ('wrong', 0.9914214015007019), ('thing', 0.991061270236969)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(model.wv.most_similar(positive=['find'], topn=5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23812126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(v, c):\n",
    "    model = Pipeline(\n",
    "        [(\"vectorizer\", v), \n",
    "         (\"classifier\", c)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print_accuracy_score_for_classes(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed7e0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer(object):\n",
    "    '''\n",
    "    Для текста усредним вектора входящих в него слов\n",
    "    '''\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.size = model.vector_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean(\n",
    "            [self.model[w] for w in words if w in self.model] \n",
    "            or [np.zeros(self.size)], axis=0)\n",
    "            for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8722ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Вычисление метрики accuracy для каждого класса\n",
    "    y_true - истинные значения классов\n",
    "    y_pred - предсказанные значения классов\n",
    "    Возвращает словарь: ключ - метка класса, \n",
    "    значение - Accuracy для данного класса\n",
    "    \"\"\"\n",
    "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # Метки классов\n",
    "    classes = np.unique(y_true)\n",
    "    # Результирующий словарь\n",
    "    res = dict()\n",
    "    # Перебор меток классов\n",
    "    for c in classes:\n",
    "        # отфильтруем данные, которые соответствуют \n",
    "        # текущей метке класса в истинных значениях\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # расчет accuracy для заданной метки класса\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # сохранение результата в словарь\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Вывод метрики accuracy для каждого класса\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('Метка \\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9f7eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary = 700\n",
    "X_train = corpus[:boundary] \n",
    "X_test = corpus[boundary:]\n",
    "y_train = newsgroups['target'][:boundary]\n",
    "y_test = newsgroups['target'][boundary:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d59516f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NoNam\\anaconda4\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.830379746835443\n",
      "1 \t 0.7798165137614679\n",
      "2 \t 0.8649289099526066\n",
      "3 \t 0.8381962864721485\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sentiment(EmbeddingVectorizer(model.wv), LogisticRegression(C=5.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47132530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
